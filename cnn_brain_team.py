# -*- coding: utf-8 -*-
"""Copy of CNN_brain_team.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nRHXuWu5gYRxLdwt-MZKcq6ch9uO9Hb2
"""

import tensorflow as tf
import sys
from dipy.io.streamline import load_tractogram
from matplotlib import pyplot
from keras.datasets import cifar10
from tensorflow.keras.utils import to_categorical
from keras.models import Sequential
from keras.layers import Conv2D
from keras.layers import MaxPooling2D
from keras.layers import Dense, Dropout
from keras.layers import Flatten
from keras.optimizers import SGD
from tensorflow.keras import datasets, layers, models
import matplotlib.pyplot as plt
import numpy as np
import os

def get_fibermap(all_trajs, n):

    fiber_map = np.zeros([20, 40, 3]) # define empty map for each streamline - only 10 tiles
    all_fibre_map = np.zeros([len(all_trajs), 20, 40, 3]) # to store all maps
    
    for j in range(len(all_trajs)):
        
        data = all_trajs[j] # choose one streamline
        
        for i in range(3): # for each dimension in streamline
            stream = data[:,i]
            stream_rev = stream[::-1] # reverse

            block1 = np.concatenate((stream, stream_rev), axis = 0) # build blocks
            block2 = np.concatenate((stream_rev, stream), axis = 0)

            cell = np.vstack((block1, block2)) # stack vertically

            fiber_slice = np.tile(cell, (10,1)) # create fiber map 
    
            fiber_map[:,:,i] = fiber_slice # assign to map for each dimension
        
        all_fibre_map[j,:,:,:] = fiber_map # save all maps from all streamlines
        
    return all_fibre_map

from google.colab import drive # copied drive folder
drive.mount('/content/drive')

path_files      = '/content/drive/MyDrive/178950/'
#show = True

fNameRef  = 't1.nii.gz'
 
classes = ['AC', 'AF_L', 'AF_R', 'CC_MID', 'CGFP_L', 'CGFP_R', 'CGH_L', 'CGH_R', 'CGR_L', 'CGR_R', 'CG_L', 'CG_R', 'CST_L', 'CST_R', 'FA_L', 'FA_R', 'FMA', 'FMI', 'FX_L', 'FX_R', 'IFOF_L', 'IFOF_R', 'ILF_L', 'ILF_R', 'MLF_L', 'MLF_R', 'OR_L', 'OR_R', 'SLF_L', 'SLF_R', 'TAPETUM', 'UF_L', 'UF_R', 'VOF_L', 'VOF_R']
# following Jean-Bernard's LSTM code
all_trajs  = []
all_labels = []
# Reads the .tck files from each specified class
for i,c in enumerate(classes):
    # Load tractogram
    filename   = path_files+c+'_20p'+'.tck'
    print('Reading file:',filename)
    tractogram = load_tractogram(filename, path_files+fNameRef, bbox_valid_check=False)
    # Get all the streamlines
    STs = tractogram.streamlines
    all_trajs.extend(STs)
    all_labels.extend(len(STs)*[i])

len(all_trajs)

streamlines = get_fibermap(all_trajs, 10)

streamlines.shape

from sklearn.model_selection import train_test_split 

x_train, x_test, y_train, y_test = train_test_split (streamlines, np.array(all_labels) , test_size=0.2, train_size=0.8 )

y_train.shape

y_test.shape

#y_train.shape
plt.hist(y_test)
x_train.shape
y_train = np.array(y_train)
y_test = np.array(y_test)

def load_dataset():
	# load dataset
	(trainX, trainY), (testX, testY) = cifar10.load_data()
	# one hot encode target values
	trainY = to_categorical(trainY)
	testY = to_categorical(testY)
	return trainX, trainY, testX, testY
# scale pixels
def prep_pixels(train, test):
	# convert from integers to floats
	train_norm = train.astype('float32')
	test_norm = test.astype('float32')
	# normalize to range 0-1
	train_norm = train_norm / 255.0
	test_norm = test_norm / 255.0
	# return normalized images
	return train_norm, test_norm

#plot metrics
def Loss_ACC_plot(history):
	# plot loss
    plt.figure()
    plt.subplot(1,2,1)
    plt.title('Cross Entropy Loss')
    plt.plot(history.history['loss'], color='blue', label='train')
    plt.plot(history.history['val_loss'], color='orange', label='validation')
    # plot accuracy
    plt.subplot(1,2,2)
    plt.title('Classification Accuracy')
    plt.plot(history.history['accuracy'], color='blue', label='train')
    plt.plot(history.history['val_accuracy'], color='orange', label='validation')
 
# define cnn model
def define_model():
	model = Sequential()
	model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(20,40,3)))
	model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
	model.add(MaxPooling2D((2, 2)))
	model.add(Dropout(0.2))
	model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
	model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
	model.add(MaxPooling2D((2, 2)))
	model.add(Dropout(0.2))
	model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
	model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
	model.add(MaxPooling2D((2, 2)))
	model.add(Dropout(0.2))
	model.add(Flatten())
	model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))
	model.add(Dropout(0.2))
	#model.add(Dense(256, activation='relu', kernel_initializer='he_uniform'))
	#model.add(Dropout(0.2))
	#model.add(Dense(512, activation='relu', kernel_initializer='he_uniform'))
	#model.add(Dropout(0.2))
	model.add(Dense(35, activation='softmax'))
	# compile model
	opt = SGD(learning_rate=0.001, momentum=0.9)
	model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])
	return model

# -*- coding: utf-8 -*-
"""
Created on Wed Jun 23 11:27:26 2021
@author: Pablo
"""

#Ensure that we are using GPU
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
            print(tf.test.gpu_device_name())
    except RuntimeError as e:
        print(e)


#x_train, y_train, x_test, y_test = load_dataset()
# prepare pixel data
#x_train, x_test = prep_pixels(x_train, x_test)
# define model
model = define_model()
# fit model
history = model.fit(x_train, y_train, validation_split=0.2, epochs=10,shuffle = True, batch_size=64)

Loss_ACC_plot(history)

import numpy as np
y_pred = model.predict(x_test)
y_pred= np.argmax(y_pred, axis=1)

from sklearn.metrics import confusion_matrix, plot_confusion_matrix
from sklearn.metrics import accuracy_score
#y_test = np.argmax(y_test, axis=0)
cmatrix = confusion_matrix(y_test, y_pred)
print(cmatrix)
ACC = np.trace(cmatrix)/np.sum(cmatrix)
print('Testing Acc: ', ACC) #~0.80 with Pablo's, ~0.8224 adding in more dense layers

confusion_mtx = tf.math.confusion_matrix(y_test, y_pred)
plt.figure(figsize=(14, 10))
sns.heatmap(confusion_mtx, xticklabels=classes, yticklabels=classes,annot=True, fmt='g')
plt.xlabel('Prediction')
plt.ylabel('True label')
plt.show()

y_pred.shape

def plot_graphs(history, metric):
  plt.plot(history.history[metric])
  plt.plot(history.history['val_'+metric], '')
  plt.xlabel("Epochs")
  plt.ylabel(metric)
  plt.legend([metric, 'val_'+metric])

plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plot_graphs(history, 'accuracy')
plt.ylim(None, 1)
plt.subplot(1, 2, 2)
plot_graphs(history, 'loss')
plt.ylim(0, None)
plt.show()

plt.savefig('history_plot.png', facecolor = 'white', transparent = False)